<?php

return [
    'app.name' => 'AI для PHP-разработчиков (примеры)',
    'app.name_short' => 'AI для PHP-разработчиков',

    'nav.home' => 'Главная',
    'nav.introduction' => 'Введение',
    'nav.getting_started' => 'С чего начать',
    'nav.code_example' => 'Пример кода',
    'nav.code_run' => 'Запускаем код',
    'nav.part1_title' => 'Часть I. Математический язык ИИ',
    'nav.part1_what_is_model' => 'Что такое модель',
    'nav.part1_function_as_the_model' => 'Функция как основа модели',
    'nav.part1_error_as_measure_of_quality' => 'Ошибка как мера качества',
    'nav.part1_learning_as_min_error' => 'Обучение как минимизация ошибки',
    'nav.part1_vectors' => 'Векторы, размерности и пространства признаков',
    'nav.part1_distances' => 'Расстояния и сходство',
    'nav.part2_title' => 'Часть II. Обучение как оптимизация',
    'nav.part2_error_loss_functions' => 'Ошибка, loss-функции и почему они нужны',
    'nav.part2_apartment_valuation_based_on_parameters' => 'Оценка стоимости квартиры по параметрам',

    // Часть III: навигация
    'nav.part3_title' => 'Часть III. Классификация и вероятности',
    'nav.part3_probability_confidence' => 'Вероятность как степень уверенности',
    'nav.part3_logistic_regression' => 'Логистическая регрессия',
    'nav.part3_softmax_example' => 'Пример вероятности с softmax',

    // Часть IV: навигация
    'nav.part4_title' => 'Часть IV. Близость и структура данных',
    'nav.part4_knn_local_solutions' => 'Алгоритм k-ближайших соседей и локальные решения',
    'nav.part4_decision_trees_space_partitioning' => 'Decision Trees и разбиение пространства',

    'home.title' => 'Главная',
    'home.heading' => 'С чего начать',
    'home.intro' => 'Это примеры к книге по "AI для PHP-разработчиков".',
    'home.official_site' => 'Официальный сайт',
    'home.official_repo' => 'Официальный репозиторий',
    'home.all_examples' => 'Все примеры кода написаны на PHP v8.2',

    'home.disclaimer_title' => 'Отказ от ответственности',
    'home.disclaimer_p1' => 'Примеры кода, приведённые в книге "ИИ для PHP-разработчиков", предназначены только для учебных целей. Они демонстрируют концепции и техники искусственного интеллекта и машинного обучения на PHP. Эти примеры не предназначены для промышленной эксплуатации и не должны использоваться на боевых серверах или в системах, работающих с конфиденциальными данными.',
    'home.disclaimer_p2' => 'Демонстрационный код не проходил полноценное тестирование на безопасность и может содержать неточности, уязвимости, неэффективные решения и другие проблемы, которые могут представлять риск при использовании в продакшене. Он может быть не на 100% точен и не всегда соответствовать лучшим практикам. Перед использованием таких решений в реальных приложениях настоятельно рекомендуется их тщательно проверить, протестировать и обезопасить.',
    'home.disclaimer_p3' => 'Автор и издатель не несут ответственности за какие-либо инциденты безопасности, потерю данных или другие убытки, которые могут возникнуть при использовании этих примеров на боевых серверах.',

    'common.copy' => 'Копировать',
    'common.copied' => 'Cкопировано!',
    'common.example_of_use' => 'Пример использования',
    'common.run_code' => 'Запустить код',
    'common.show_code' => 'Показать код',
    'common.click_to_collapse' => 'Нажмите, чтобы свернуть',
    'common.click_to_expand' => 'Нажмите, чтобы развернуть',
    'common.collapse' => 'Свернуть',
    'common.expand' => 'Развернуть',
    'common.result' => 'Результат',
    'common.memory' => 'Память',
    'common.time' => 'Время',
    'common.time_running' => 'работы',
    'common.seconds_short' => 'сек.',
    'common.charts' => 'Графики',
    'common.regenerate' => 'Сгенерировать заново',
    'common.implementation_in_pure_php' => 'Реализация на чистом PHP',
    'common.debug' => 'Дебаг',
    'common.debug_traceback' => 'Трассировка отладки',
    'common.show_debug' => 'Показать дебаг',
    'common.open_in_full_screen' => 'Открыть в полноэкранном режиме',
    'common.git_repository' => 'GitHub Репозиторий',
    'common.back' => 'Назад',
    'common.example' => 'Пример',

    // Экосистема ML в PHP (intro)
    'ml_ecosystem.title' => 'Экосистема ML в PHP',
    'ml_ecosystem.breadcrumb' => 'Экосистема ML в PHP',
    'ml_ecosystem.sample_phpml_title' => 'Пример на PHP-ML',
    'ml_ecosystem.sample_rubix_title' => 'Пример на RubixML',
    'ml_ecosystem.examples_heading' => 'Учебные примеры',
    'ml_ecosystem.examples_intro' => 'Эти примеры помогут вам понять, как можно использовать ML в PHP. Они не являются полноценными приложениями, но помогут вам понять основы работы с ML в PHP.',
    'ml_ecosystem.rubix_intro' => 'Rubix поддерживает классификацию, регрессию, кластеризацию и работу с датасетами как с first-class объектами. Посмотрим, как это выглядит на практике. <br>Допустим, у нас есть данные для бинарной классификации. Код ниже также обучает классификатор k-ближайших соседей (k-NN), но уже на данных роста и веса с метками пола, а затем предсказывает метку $M$ для нового человека. Для параметров  модель возвращает $[172, 68]$, так как среди 3 ближайших соседей большинство с этой меткой.',
    'ml_ecosystem.phpml_intro' => 'Типичный сценарий: у вас есть признаки из базы данных, вы хотите быстро обучить модель для классификации или регрессии, сохранить ее и использовать в runtime без внешних сервисов. <br>Рассмотрим простой и показательный пример. В нём мы обучаем классификатор k-ближайших соседей (k-NN) на небольшом наборе точек, каждая из которых принадлежит одному из двух классов – $a$ или $b$. После обучения модель должна определить, к какому классу относится новая точка. Мы задаём обучающую выборку в виде координат на плоскости и соответствующие им метки классов. Для точки $[3, 2]$ алгоритм возвращает класс $b$, потому что её ближайшие соседи в обучающей выборке относятся именно к этому классу.',

    'what_is_model.heading' => 'Что такое модель в математическом смысле',
    'what_is_model.function_as_basis_of_model' => 'Функция как основа модели',
    'what_is_model.error_as_measure_of_quality' => 'Ошибка как мера качества',
    'what_is_model.description1' => 'Допустим, мы хотим предсказывать цену квартиры по ее площади. В простейшем случае модель можно взять линейную: $ŷ = w x + b$ <br><br>Это уже полноценная модель. Она говорит: "Цена ($ŷ$) приблизительно равна площади ($x$), умноженной на некоторый коэффициент ($w$), плюс некоторое смещение ($b$)". <br><br>Если переписать это на PHP, получится почти тривиальный код:',
    'what_is_model.description2' => 'Предсказание цены квартиры по ее площади. Используем линейную функцию: $ŷ = w x + b$',
    'what_is_model.explanation_simple' => 'Пояснение: $2 * 3 + 0 = 6$ <br>По формуле: $ŷ = 2 x + b$',
    'what_is_model.intro' => 'Когда мы говорим о модели в машинном обучении, полезно сразу отбросить все ассоциации с "искусственным интеллектом" и сложными абстракциями. В математическом смысле модель — это функция: она принимает входные данные и возвращает результат. Главное отличие в том, что такая функция не жёстко задана раз и навсегда, а имеет настраиваемые параметры.',
    'what_is_model.link_function_as_model' => 'Функция как основа модели',
    'what_is_model.link_error_as_quality' => 'Ошибка как мера качества',
    'what_is_model.link_learning_as_min_error' => 'Обучение как минимизация ошибки',

    'what_is_model.learning_as_min_error.title' => 'Обучение как минимизация ошибки',
    'what_is_model.learning_as_min_error.intro1' => 'Если мы умеем измерять ошибку (loss), то обучение модели можно понимать очень просто: мы меняем параметры модели так, чтобы это число уменьшалось. Модель сама по себе не "понимает" задачу – она всего лишь минимизирует выбранную нами функцию ошибки.',
    'what_is_model.learning_as_min_error.intro2' => 'Ниже – минимальный пример: у нас есть два наблюдения и линейная модель $ŷ = w x + b$. Сначала параметры плохие, и loss большой. Затем мы подбираем $w$ так, чтобы предсказания стали ближе к реальности, и loss заметно уменьшается.',
    'what_is_model.learning_as_min_error.explanation' => 'Идея обучения: повторять шаги изменения параметров (например, градиентным спуском), пока средняя ошибка на данных не станет достаточно маленькой.',

    'what_is_model.error_measure.intro1' => 'Ошибка — это функция (общепринятое название — loss‑функция), которая сравнивает предсказание модели с реальным значением и возвращает число, показывающее, насколько мы ошиблись. Чем меньше это число, тем лучше модель. Например, самая простая ошибка — разница между предсказанием и реальностью: $ŷ - y$.',
    'what_is_model.error_measure.intro2' => 'На практике чаще используют квадрат ошибки (Squared Error или SE), потому что он всегда положительный и сильнее наказывает большие промахи: $(ŷ − y)^2$.',
    'what_is_model.error_measure.explanation' => 'Пояснение: −3 ⇒ 7 − 10 = −3<br>Пояснение: 4 ⇒ (6 − 4)² = 2² = 4',

    'errors_loss.heading' => 'Ошибка, loss-функции и зачем они нужны',
    'errors_loss.intro' => 'Любая модель машинного обучения сводится к простой идее: она пытается приблизить реальность функцией. А значит, между тем, что есть на самом деле, и тем, что говорит модель, всегда будет расхождение. Это расхождение мы и называем ошибкой. Важно сразу понять одну вещь: модель не знает, что такое "хорошо" и "плохо". Она не понимает смысл задачи. Всё, что она умеет — уменьшать число, которое мы ей дали. Это число и есть loss. Формально ошибка — это отклонение между $y$ и $\\hat{y}$, а loss — функция, которая превращает это отклонение в число, удобное для оптимизации.',
    'errors_loss.intro2' => 'Ниже мы разберём несколько коротких, но показательных кейсов на PHP, которые шаг за шагом связывают формулы с реальными задачами:',
    'errors_loss.case1_title' => 'Кейс 1. MSE и цена большого промаха',
    'errors_loss.case1.intro1' => 'Представим сервис оценки стоимости квартир. Ничего сложного: на вход подаётся площадь, на выходе – прогнозируемая цена. Это типичная задача регрессии, и для неё почти автоматически выбирают MSE. Но именно здесь хорошо видно, какую цену мы платим за такой выбор.',
    'errors_loss.case1.intro2' => 'Реализуем MSE буквально в несколько строк, без каких-либо библиотек и затем испортим картину всего одной точкой. Пусть в данных появилась странная квартира: либо ошибка в базе, либо уникальный объект, либо просто неудачный прогноз.',
    'errors_loss.case1.mse_description' => 'MSE — среднеквадратичная ошибка, одна из самых распространённых loss‑функций для задач регрессии. Она измеряет средний квадрат разницы между предсказанием модели и истинным значением:<br><br>$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$',
    'errors_loss.case1.explanation' => 'Нормальный MSE: $4$<br>После добавления выброса MSE очень сильно изменился — 4820.<br>Почему так происходит, легко увидеть на уровне одной формулы (по выбросу): $(300 - 130)^2 = 170^2 = 28900$.',
    'errors_loss.case2_title' => 'Кейс 2. Выбор модели через loss-функцию',
    'errors_loss.case2.block_intro' => 'Этот кейс логически продолжает предыдущий. Если там мы смотрели, как одна ошибка может испортить всю картину, то здесь ответим на другой практический вопрос: как формально выбрать лучшую модель, когда вариантов несколько и "на глаз" всё выглядит почти одинаково.<br><br><b>Цель кейса</b>:<br>Показать, что loss-функция превращает субъективное "кажется, эта модель лучше" в измеримый критерий, по которому можно принимать решение.<br><br><b>Сценарий</b>:<br>Представим задачу прогнозирования спроса на продукт. У нас есть исторические данные и два варианта модели:',
    'errors_loss.case2.model_a' => 'Модель A – простая линейная, понятная и стабильная',
    'errors_loss.case2.model_b' => 'Модель B – чуть более сложная, с дополнительными параметрами',
    'errors_loss.case2.models_trained_text' => 'Обе модели уже обучены. Мы не обсуждаем, как именно они устроены – в этом кейсе важно только одно: какая из них ошибается меньше.',
    'errors_loss.case2.result_paragraph1' => 'После вычисления MSE мы получаем два конкретных числа. Одно из них меньше – и это единственный формальный аргумент, который действительно важен для обучения и выбора модели.',
    'errors_loss.case2.result_paragraph2' => 'Даже если разница между MSE невелика, она всё равно отражает систематическое преимущество одной модели над другой в рамках выбранной философии ошибки.',
    'errors_loss.case2.result_paragraph3' => 'Даже если визуально разница кажется небольшой, loss даёт численное основание для выбора.',
    'errors_loss.case3_title' => 'Кейс 3. Log loss и уверенность классификатора',
    'errors_loss.case3.intro1' => 'В задачах классификации нас волнует не только то, какой класс предсказывает модель, но и насколько она в этом уверена. Log loss (кросс‑энтропийный loss) естественным образом наказывает чрезмерно уверенные ошибки и поощряет хорошо откалиброванные вероятности.',
    'errors_loss.case3.intro2' => 'Две модели могут давать одинаковые итоговые метки классов при пороге 0.5, но при этом одна из них будет сильно лучше с точки зрения вероятностей. Log loss делает эту разницу видимой в виде одного числа.',
    'errors_loss.case3.logloss_description' => 'Для бинарной классификации log loss определяется так:<br><br>$\text{LogLoss} = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(p_i) + (1 - y_i) \log(1 - p_i)]$, где $y_i$ — истинная метка (0 или 1), а $p_i$ — предсказанная вероятность класса 1.',
    'errors_loss.case3.explanation' => 'Даже если предсказанные классы совпадают, модель, дающая вероятности ближе к реальности, будет иметь меньший log loss. Особо сильно наказываются уверенные, но ошибочные прогнозы.',
    'errors_loss.case3.per_sample_heading' => 'Вклад отдельных примеров в Log loss (модель A)',
    'errors_loss.case3.curve_heading' => 'Log loss как функция вероятности (y = 1)',
    'errors_loss.case3.per_sample_dataset_label' => 'Per-sample Log loss',
    'errors_loss.case3.sample_index_label' => 'Sample index',
    'errors_loss.case4_title' => 'Кейс 4. Одинаковая точность – разный log loss',
    'errors_loss.case4.intro1' => 'Точность (accuracy) отвечает только на вопрос: сколько раз модель угадала класс при выбранном пороге (например, 0.5). Но она вообще не учитывает, насколько уверенно модель делает предсказания.',
    'errors_loss.case4.intro2' => 'В этом кейсе мы возьмём один и тот же набор истинных меток и две модели, которые дают одинаковые классы при пороге 0.5. Однако одна модель предсказывает вероятности ближе к 0 и 1 (более уверенно) и получит меньший log loss, а другая остаётся ближе к 0.5 и будет наказана большим log loss.',
    'errors_loss.case4.explanation' => 'Вывод: две модели могут иметь одинаковую точность, но разный log loss. Log loss смотрит на качество вероятностей (калибровку) и сильнее штрафует случаи, когда модель уверена, но ошибается.',
    'errors_loss.case5_title' => 'Кейс 5. Обучение модели как минимизация ошибки',

    'linear_regression.heading' => 'Линейная регрессия как базовая модель',
    'linear_regression.intro1' => 'Линейная регрессия – это та точка, с которой удобно начинать разговор про машинное обучение. Не потому, что она "простая", а потому, что в ней уже есть почти всё: модель как функция, параметры, ошибка, оптимизация и геометрический смысл. Если понять линейную регрессию, дальше большинство моделей будут восприниматься как её усложнения.',
    'linear_regression.intro2' => 'В следующих кейсах мы будем рассматривать линейную регрессию не как учебный алгоритм, а как инженерный инструмент.',
    'linear_regression.case1_title' => 'Кейс 1. Оценка стоимости квартиры по параметрам',
    'linear_regression.case2_title' => 'Кейс 2. Прогноз времени выполнения задачи разработчиком',
    'linear_regression.case3_title' => 'Кейс 3. Прогноз потребления ресурсов сервера',
    'linear_regression.case4_title' => 'Кейс 4. Оценка вероятного чека клиента',
    'linear_regression.case5_title' => 'Кейс 5. Прогноз зарплаты по рынку',
    'linear_regression.case1.php_impl_intro' => 'Начнём с варианта без библиотек. Это полезно не для продакшена, а для понимания. Мы будем использовать градиентный спуск, матрицу признаков $X$ размером $N$ x $4$ и вектор весов $w$ длины $4$. Bias добавим как дополнительный признак со значением 1.',
    'linear_regression.case1.rubix_impl_title' => 'Реализация на RubixML',
    'linear_regression.case1.rubix_impl_intro' => 'Теперь сделаем то же самое, но так, как это обычно делается в реальных проектах. Используем линейную регрессию методом наименьших квадратов. Здесь библиотека сама решает задачу оптимизации и подбирает веса аналитически.',
    'linear_regression.case1.php_result_intro' => 'В блоке выше показан результат работы скрипта: предсказанная стоимость квартиры по следующим признакам:',
    'linear_regression.case1.feature_area' => 'Площадь квартиры: 60 м²',
    'linear_regression.case1.feature_rooms' => 'Количество комнат: 5',
    'linear_regression.case1.feature_bathrooms' => 'Количество ванных комнат: 4',
    'linear_regression.case1.feature_floors' => 'Количество этажей: 12',
    'linear_regression.case1.feature_bias' => 'Отклонение: 1',

    // Linear regression: Case 2 (developer task completion time)
    'linear_regression.case2.intro1' => 'В реальных командах вопрос "сколько времени займёт задача?" звучит почти каждый день. От этого зависят сроки релиса, загрузка разработчиков и ожидания бизнеса. Обычно оценки даются «на глаз» — по опыту тимлида или самого исполнителя. Линейная регрессия позволяет формализовать этот процесс и получить базовую модель, которая предсказывает время на основе признаков задачи.',
    'linear_regression.case2.intro2' => 'Представим, что у нас есть исторические данные по задачам: для каждой задачи мы знаем, сколько часов она фактически заняла, и вектор признаков $\\mathbf{x} = (x_1, x_2, x_3, x_4)$, где:',
    'linear_regression.case2.feature_x1' => '$x_1$ — story points (оценка сложности задачи);',
    'linear_regression.case2.feature_x2' => '$x_2$ — количество файлов, затронутых изменением;',
    'linear_regression.case2.feature_x3' => '$x_3$ — количество строк изменений (diff по коду);',
    'linear_regression.case2.feature_x4' => '$x_4$ — опыт разработчика (например, в годах или закодированный уровень junior/middle/senior);',
    'linear_regression.case2.formula' => 'Мы хотим предсказывать целевую величину — фактическое время выполнения задачи в часах. Линейная регрессия в этом случае задаёт простую, интерпретируемую модель вида $ŷ = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4$, где компоненты $\\mathbf{x}$ описывают задачу, а коэффициенты $w_1, \\dots, w_4$ и смещение $w_0$ мы подбираем по историческим данным.',

    'linear_regression.case2.rubix_intro' => 'В этом примере мы используем линейную регрессию Ridge из RubixML, чтобы предсказать время выполнения задачи разработчиком на основе нескольких числовых признаков и одновременно посмотреть на веса модели и bias.',
    'linear_regression.case2.explain_intro' => 'Теперь модель можно объяснить (вес — это coefficient):',
    'linear_regression.case2.explain_item1' => 'вес при story points показывает, сколько часов в среднем добавляет один SP;',
    'linear_regression.case2.explain_item2' => 'вес при количестве файлов отражает накладные расходы на контекст;',
    'linear_regression.case2.explain_item3' => 'вес при количестве строк часто коррелирует с объёмом ручной работы;',
    'linear_regression.case2.explain_item4' => 'отрицательный вес при опыте разработчика — ожидаемый и логичный результат (чем больше опыт, тем меньше времени обычно нужно на ту же задачу, поэтому связь обратная и вес получается отрицательный);',
    'linear_regression.case2.explain_outro' => 'Такую модель уже можно обсуждать с командой и осознанно корректировать признаки.',

    // Линейная регрессия: Кейс 3 (прогноз потребления ресурсов сервера)
    'linear_regression.case3.intro1' => 'На продакшене часто важнее не только время ответа, но и то, выдержит ли инфраструктура нагрузку: насколько вырастет загрузка CPU, не упрёмся ли в память, не начнёт ли база задыхаться. Линейная регрессия позволяет быстро построить интерпретируемую модель, которая по нескольким простым метрикам трафика даёт прогноз нагрузки на сервер.',
    'linear_regression.case3.intro2' => 'Представим, что у нас есть исторические данные мониторинга. Для каждого временного окна мы знаем вектор признаков $\\mathbf{x} = (x_1, x_2, x_3, x_4, x_5)$, где:',
    'linear_regression.case3.feature_x1' => '$x_1$ — количество запросов в минуту;',
    'linear_regression.case3.feature_x2' => '$x_2$ — средний размер ответа (КБ);',
    'linear_regression.case3.feature_x3' => '$x_3$ — число активных пользователей на сайте;',
    'linear_regression.case3.feature_x4' => '$x_4$ — количество фоновых cron-задач за интервал;',
    'linear_regression.case3.feature_x5' => '$x_5$ — час суток (например, от 0 до 23);',
    'linear_regression.case3.formula' => 'Целевая переменная — загрузка CPU в процентах. Линейная регрессия в этом случае задаёт модель вида $ŷ = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4 + w_5 x_5$, где коэффициенты $w_1, \\dots, w_5$ и смещение $w_0$ подбираются по историческим данным мониторинга.',
    'linear_regression.case3.rubix_intro' => 'В этом примере мы используем линейную регрессию из RubixML, чтобы по простому набору метрик трафика спрогнозировать загрузку CPU и одновременно посмотреть на веса модели и bias.',
    'linear_regression.case3.explain_intro' => 'После обучения модель можно интерпретировать так же, как и в кейсе про оценки задач:',
    'linear_regression.case3.explain_item1' => 'вес при количестве запросов в минуту показывает, насколько чувствителен CPU к росту входящего трафика;',
    'linear_regression.case3.explain_item2' => 'вес при среднем размере ответа отражает влияние «тяжёлых» ответов (рендеринг шаблонов, большие JSON/HTML);',
    'linear_regression.case3.explain_item3' => 'вес при числе активных пользователей косвенно учитывает одновременные сессии и конкуренцию за ресурсы;',
    'linear_regression.case3.explain_item4' => 'вес при количестве cron-задач показывает влияние фоновой нагрузки (бэкапы, отчёты, пересчёты);',
    'linear_regression.case3.explain_item5' => 'вес при часе суток позволяет модели учитывать суточные паттерны нагрузки (пики днём, провалы ночью);',
    'linear_regression.case3.explain_outro' => 'Такую модель удобно использовать как быструю прикидку "выдержит ли сервер ещё +X% трафика" и как отправную точку для более сложных моделей и алертов в системе мониторинга.',

    // Кейс 1: UI графиков (code-run.php)
    'linear_regression.case1.chart_price_vs_area' => 'Цена vs Площадь',
    'linear_regression.case1.chart_price_vs_floor' => 'Цена vs Этаж',
    'linear_regression.case1.chart_price_vs_distance' => 'Цена vs Расстояние до центра',
    'linear_regression.case1.chart_price_vs_age' => 'Цена vs Возраст дома',
    'linear_regression.case1.chart_xlabel_area' => 'Площадь (м²)',
    'linear_regression.case1.chart_xlabel_floor' => 'Этаж',
    'linear_regression.case1.chart_xlabel_distance' => 'Расстояние до центра (км)',
    'linear_regression.case1.chart_xlabel_age' => 'Возраст здания (лет)',
    'linear_regression.case1.chart_ylabel_price' => 'Цена ($)',
    'linear_regression.case1.chart_regression_label' => 'Линия регрессии',
    'linear_regression.case1.controls_chart_type' => 'Тип графика',

    'gradient_descent.heading' => 'Градиентный спуск на пальцах',
    'gradient_descent.implementation' => 'Реализация градиентного спуска',
    'gradient_descent.minimal_example_intro' => 'Начнём с минимального примера: один признак, один вес. Оценка стоимости квартиры по её площади.',
    'gradient_descent.intro1' => 'Градиентный спуск, метод градиентного спуска – численный метод нахождения локального минимума или максимума функции с помощью движения вдоль градиента, один из основных численных методов современной оптимизации.',
    'gradient_descent.intro2' => 'Когда в машинном обучении говорят "обучение модели", почти всегда имеют в виду одно и то же: мы хотим подобрать параметры так, чтобы ошибка стала как можно меньше. Какая именно модель – линейная регрессия, логистическая, нейросеть – не так важно. Важнее то, что за кулисами почти всегда работает один и тот же механизм – градиентный спуск.',
    'gradient_descent.implementation_link' => 'Реализация градиентного спуска',
    'gradient_descent.sample1_title' => 'Пример 1. Траектория параметра',
    'gradient_descent.sample2_title' => 'Пример 2. Влияние learning rate',
    'gradient_descent.sample3_title' => 'Пример 3. Плато и почти нулевой градиент',
    'gradient_descent.sample4_title' => 'Пример 4. Batch и стохастический спуск',
    'gradient_descent.result_hint' => 'Для этих данных результат будет близок к:',
    'gradient_descent.debug_title' => 'Отладка градиентного спуска',
    'gradient_descent.learning_rate' => 'Скорость обучения',
    'gradient_descent.epochs' => 'Эпохи',
    'gradient_descent.impl_php_from_scratch' => 'Реализация на PHP – с нуля',
    'gradient_descent.impl_php_vector_version' => 'Реализация на PHP – векторная версия',
    'gradient_descent.more_features_vectors_hint' => 'Когда признаков больше, удобнее мыслить векторами.',

    // Часть III: Вероятность как степень уверенности
    'probability_confidence.intro' => 'Когда разработчики слышат слово "вероятность", в голове часто всплывают игральные кости, подбрасывание монетки и школьная формула "благоприятные исходы делить на все возможные". Это полезная, но очень узкая картинка. В машинном обучении и в прикладной аналитике вероятность почти всегда означает другое – степень нашей уверенности в утверждении, исходя из имеющихся данных.',
    'probability_confidence.link_softmax_example' => 'Пример вероятности с softmax',
    'probability_confidence.case1_title' => 'Кейс 1. Фильтр спама: вероятность ≠ решение',
    'probability_confidence.case2_title' => 'Кейс 2. Медицинский тест: обновление уверенности',
    'probability_confidence.case3_title' => 'Кейс 3. Многоклассовая классификация и softmax',
    'probability_confidence.case4_title' => 'Кейс 4. Переуверенная модель как сигнал проблемы',
    'probability_confidence.case5_title' => 'Кейс 5. Обновление уверенности при новых данных',

    'probability_confidence.logits_paragraph' => 'Во многих моделях машинного обучения выходом являются не вероятности, а так называемые оценки (logits). Это просто числа, отражающие относительную уверенность модели в каждом варианте. Они могут быть любыми – положительными, отрицательными, большими или маленькими – и сами по себе не интерпретируются как вероятность. Чтобы превратить такие оценки в корректные вероятности, используется функция softmax.',
    'probability_confidence.softmax_result_explanation' => 'Теперь мы получили корректное распределение вероятностей: каждое значение находится в диапазоне от 0 до 1; сумма всех значений равна 1; числа можно интерпретировать как степень уверенности модели.',

    // Часть III: кейсы по логистической регрессии
    'logistic_regression.intro' => 'До этого мы говорили о логистической регрессии как о модели: формула, sigmoid, вероятность, decision boundary. Всё это важно, но само по себе остаётся теорией. Кейсы нужны, чтобы показать, как эта модель работает на реальных задачах. Здесь не будет "идеальных" примеров. Данные могут быть простыми или шумными, признаки – очевидными или странными, решения – не всегда однозначными. Это нормально. Именно так логистическая регрессия используется на практике.',
    'logistic_regression.case1.php_intro' => 'Отток пользователей – одна из самых типичных задач бинарной классификации. Пользователь либо остается в продукте, либо уходит. При этом нас интересует не просто ответ «да или нет», а вероятность ухода: насколько риск высок и стоит ли реагировать.',
    'logistic_regression.case1.php_intro2' => 'Именно поэтому логистическая регрессия здесь подходит особенно хорошо. Построим при помощи чистого PHP простую модель, которая по поведению пользователя оценивает вероятность его ухода и помогает понять, как эта вероятность формируется из признаков.',
    'logistic_regression.case1_title' => 'Кейс 1. Логистическая регрессия для оттока клиентов',
    'logistic_regression.case2_title' => 'Кейс 2. Подписка на рассылку',
    'logistic_regression.case3_title' => 'Кейс 3. Спам или не спам',
    'logistic_regression.case4_title' => 'Кейс 4. Клик по рекламе (CTR)',
    'logistic_regression.case5_title' => 'Кейс 5. Одобрение кредита',
    'logistic_regression.case6_title' => 'Кейс 6. Фрод или нормальная транзакция',
    'logistic_regression.case7_title' => 'Кейс 7. Медицинский скрининг',
    'logistic_regression.case8_title' => 'Кейс 8. Технический дефект оборудования',
    'logistic_regression.case1.rubix_intro' => 'В реальных проектах логистическую регрессию редко пишут вручную. Гораздо удобнее использовать готовые библиотеки.',
    'logistic_regression.case1.rubix_intro2' => 'Тот же самый кейс с RubixML выглядит заметно короче и ближе к тому, как это делается в продакшене:',
    'why_naive_bayes_works.title' => 'Почему наивный Байес работает',
    'why_naive_bayes_works.case1.title' => 'Кейс 1. Категориальные признаки и частоты',
    'why_naive_bayes_works.case1.php_intro' => 'В этом кейсе мы посмотрим на самый простой наивный Байес с категориальными признаками. Мы явно посчитаем частоты по классам, превратим их в вероятности и классифицируем нового пользователя.',
    'why_naive_bayes_works.case1.rubix_intro' => 'Затем реализуем тот же самый пример через RubixML и увидим, как та же логика выражается на уровне библиотеки.',
    'why_naive_bayes_works.case1.php_run_intro' => 'В этом примере мы руками реализуем наивный Байес для пары простых категориальных признаков и посмотрим, какие логарифмы вероятностей получаются для классов.',
    'why_naive_bayes_works.case1.php_result_explanation' => 'Результат выводится как логарифмы оценок вероятности для каждого класса. Чем значение больше (то есть ближе к 0), тем класс вероятнее. Мы суммируем лог‑вероятности по признакам и применяем сглаживание Лапласа, поэтому после сортировки первым стоит предсказанный класс.',
    'why_naive_bayes_works.case1.rubix_run_intro' => 'Здесь тот же кейс реализован через RubixML: мы обучаем наивный Байес на тех же данных и смотрим, какой класс модель назначит новому объекту.',
    'why_naive_bayes_works.case1.rubix_result_explanation' => 'RubixML выводит предсказанный класс для заданного объекта. Под капотом применяется та же идея наивного Байеса (априорная вероятность класса × условные вероятности признаков), поэтому результат должен совпадать с реализацией на чистом PHP при тех же данных.',

    // Part IV: k-NN and local solutions
    'knn_local_solutions.index.intro' => 'Алгоритм k-ближайших соседей (k-Nearest Neighbors, k-NN) – один из самых интуитивных и при этом фундаментальных алгоритмов машинного обучения. Он почти не делает предположений о данных, не обучает параметрическую модель, а хранит обучающие данные и опирается на простую, почти геометрическую идею: похожие объекты должны иметь похожие ответы.',
    'knn_local_solutions.index.case1' => 'Кейс 1. Классификация клиента по поведению',
    'knn_local_solutions.index.case2' => 'Кейс 2. Регрессия: оценка цены квартиры',
    'knn_local_solutions.index.case3' => 'Кейс 3. Классификация с RubixML',
    'knn_local_solutions.case1.intro' => 'В этом кейсе мы разберём один из самых простых и наглядных сценариев применения алгоритма k-ближайших соседей – классификацию пользователя на основе его поведения на сайте. Пример намеренно упрощён, чтобы внимание было сосредоточено не на инфраструктуре, а на логике принятия решения.',

    'decision_trees_space_partitioning.index.intro' => 'Деревья решений – один из самых интуитивных и одновременно мощных алгоритмов машинного обучения. Их любят за наглядность, за близость к человеческой логике "если – то" и за то, что результат можно объяснить не только инженеру, но и бизнесу, менеджеру или клиенту. В отличие от многих других моделей, дерево решений не выглядит как чёрный ящик – оно буквально рисуется на бумаге.',
    'decision_trees_space_partitioning.index.case1' => 'Кейс 1. Учебное дерево решений на чистом PHP',
    'decision_trees_space_partitioning.index.case2' => 'Кейс 2. Классификация с помощью RubixML',
    'decision_trees_space_partitioning.index.case3' => 'Кейс 3. Когда дерево удобно в реальном продукте',
    'decision_trees_space_partitioning.case1.intro' => 'В данном случае мы строим минимальное дерево решений для наглядного примера: вычисляем энтропию и прирост информации для простого разделения, выбираем оптимальный порог для признака и показываем, как набор данных разделяется на левую и правую ветви.',
    'decision_trees_space_partitioning.case1.diagram_default' => 'Дерево решений',
    'decision_trees_space_partitioning.case1.diagram_all_data' => 'Все данные',
    'decision_trees_space_partitioning.case1.diagram_left_branch' => 'Левая ветка',
    'decision_trees_space_partitioning.case1.diagram_right_branch' => 'Правая ветка',
    'decision_trees_space_partitioning.case1.diagram_yes' => 'Да',
    'decision_trees_space_partitioning.case1.diagram_no' => 'Нет',
    'decision_trees_space_partitioning.case1.diagram_graph_label' => 'Граф',
    'decision_trees_space_partitioning.case1.diagram_class_label' => 'Класс',
];
