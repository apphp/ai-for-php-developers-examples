<?php

return [
    'app.name' => 'AI for PHP Developers (examples)',
    'app.name_short' => 'AI for PHP Developers',

    'nav.home' => 'Home',
    'nav.introduction' => 'Introduction',
    'nav.getting_started' => 'Getting Started',
    'nav.code_example' => 'Code example',
    'nav.code_run' => 'Run Code',
    'nav.part1_title' => 'Part I. The Mathematical Language of AI',
    'nav.part1_what_is_model' => 'What is a model',
    'nav.part1_function_as_the_model' => 'Function as the basis of the model',
    'nav.part1_error_as_measure_of_quality' => 'Error as a measure of quality',
    'nav.part1_learning_as_min_error' => 'Learning as minimizing error',
    'nav.part1_vectors' => 'Vectors, dimensions and feature spaces',
    'nav.part1_distances' => 'Distances and similarity',
    'nav.part2_title' => 'Part II. Learning as Optimization',
    'nav.part2_error_loss_functions' => 'Error, loss functions, and why they are needed',
    'nav.part2_apartment_valuation_based_on_parameters' => 'Apartment valuation based on parameters',

    // Part III navigation
    'nav.part3_title' => 'Part III. Classification and probabilities',
    'nav.part3_probability_confidence' => 'Probability as degree of confidence',
    'nav.part3_logistic_regression' => 'Logistic regression',
    'nav.part3_softmax_example' => 'Probability example with softmax',

    // Part IV navigation
    'nav.part4_title' => 'Part IV. Proximity and data structure',
    'nav.part4_knn_local_solutions' => 'The k-nearest neighbors algorithm and local solutions',
    'nav.part4_decision_trees_space_partitioning' => 'Decision Trees and space partitioning',

    // Part V navigation
    'nav.part5_title' => 'Part V. Text as mathematics',
    'nav.part5_why_do_words_turn_into_numbers' => 'Why do words turn into numbers',
    'nav.part5_bag_of_words_and_tf_idf' => 'Bag of Words and TF–IDF',
    'nav.part5_embeddings_as_continuous_spaces_of_meaning' => 'Embeddings as continuous spaces of meaning',
    'nav.part5_transformers_from_static_vectors_to_understanding_meaning' => 'Transformers and context: from static vectors to understanding meaning',
    'nav.part5_hands_on_embedding_in_php_with_transformers' => 'Hands-on: embeddings in PHP with transformers',
    'nav.part5_retrieval_augmented_generation_as_engineering_system' => 'RAG: Retrieval-Augmented Generation as an engineering system',

    'home.title' => 'Home',
    'home.heading' => 'Getting Started',
    'home.intro' => 'This is an examples for "AI for PHP Developers" book.',
    'home.official_site' => 'Official site',
    'home.official_repo' => 'Official repository',
    'home.all_examples' => 'All code examples are written in PHP v8.2',

    'home.disclaimer_title' => 'Disclaimer',
    'home.disclaimer_p1' => 'The code examples provided for the book "AI for PHP Developers" are intended for educational purposes only. These examples are designed to illustrate concepts and techniques in artificial intelligence and machine learning using PHP. They are not suitable for production use and should not be deployed on live servers or systems that handle sensitive data.',
    'home.disclaimer_p2' => 'The demo code has not been subjected to rigorous security testing and may contain inaccuracies, vulnerabilities, inefficiencies, or other issues that could pose security risks if used in production environments. As such, it may not be 100% accurate or reflect best practices. We strongly advise readers to thoroughly review, test, and secure any implementation of the techniques demonstrated in this book before using them in real-world applications.',
    'home.disclaimer_p3' => 'The author and publisher are not responsible for any security breaches, data losses, or other damages that may result from using these examples on production servers.',

    'common.copy' => 'Copy',
    'common.copied' => 'Copied!',
    'common.example_of_use' => 'Example of use',
    'common.run_code' => 'Run Code',
    'common.show_code' => 'Show Code',
    'common.click_to_collapse' => 'Click to collapse',
    'common.click_to_expand' => 'Click to expand',
    'common.expand' => 'Expand',
    'common.collapse' => 'Collapse',
    'common.result' => 'Result',
    'common.memory' => 'Memory',
    'common.time' => 'Time',
    'common.time_running' => 'running',
    'common.seconds_short' => 'sec.',
    'sec' => 'sec.',
    'common.charts' => 'Charts',
    'common.regenerate' => 'Re-generate',
    'common.implementation_in_pure_php' => 'Implementation in pure PHP',
    'common.debug' => 'Debug',
    'common.debug_traceback' => 'Debug traceback',
    'common.show_debug' => 'Show debug',
    'common.open_in_full_screen' => 'Open in full screen',
    'common.git_repository' => 'GitHub Repository',
    'common.back' => 'Back',
    'common.example' => 'Example',
    'common.documents' => 'Documents',

    // ML ecosystem in PHP (intro section)
    'ml_ecosystem.title' => 'ML Ecosystem in PHP',
    'ml_ecosystem.breadcrumb' => 'ML Ecosystem in PHP',
    'ml_ecosystem.sample_phpml_title' => 'Example with PHP-ML',
    'ml_ecosystem.sample_rubix_title' => 'Example with RubixML',
    'ml_ecosystem.sample_transformers_title' => 'Example with TransformersPHP',
    'ml_ecosystem.sample_llphant_title' => 'Example with LLPhant',
    'ml_ecosystem.examples_heading' => 'Learning examples',
    'ml_ecosystem.examples_intro' => 'These examples will help you understand how you can use ML in PHP. They are not full-fledged applications, but they will help you understand the basics of working with ML in PHP.',
    'ml_ecosystem.rubix_intro' => 'Rubix supports classification, regression, clustering, and working with datasets as first-class objects. Let\'s see how this looks in practice. Let\'s say we have data for binary classification. <br>The code below also trains a k-nearest neighbors (k-NN) classifier, but this time on height and weight data with gender labels, and then predicts the label $M$ for a new person. The model returns $[172, 68]$ for the parameters, since most of the 3 nearest neighbors have this label.',
    'ml_ecosystem.phpml_intro' => 'A typical scenario: you have features from a database, you want to quickly train a model for classification or regression, save it, and use it in runtime without external services. <br>Let\'s consider a simple and illustrative example. In it, we train a k-nearest neighbors (k-NN) classifier on a small set of points, each of which belongs to one of two classes — $a$ or $b$. After training, the model must determine which class a new point belongs to. We specify the training set as coordinates on a plane and the corresponding class labels. For the point $[3, 2]$, the algorithm returns class b because its nearest neighbors in the training set belong to this class.',
    'ml_ecosystem.transformers_intro' => 'TransformersPHP gives you a simple pipeline API for tasks like sentiment analysis, text classification or semantic comparison. The snippet below runs a sentiment model on two short phrases and shows the resulting label and score.',
    'ml_ecosystem.llphant_intro' => 'LLPhant is a lightweight framework to call LLMs from PHP. The snippet below sends a single prompt to OpenAI via LLPhant and prints the model response.',

    'what_is_model.heading' => 'What is a model in the mathematical sense',
    'what_is_model.function_as_basis_of_model' => 'Function as the basis of the model',
    'what_is_model.error_as_measure_of_quality' => 'Error as a measure of quality',
    'what_is_model.description1' => 'Let\'s say we want to predict the price of an apartment based on its square footage. In the simplest case, we can use a linear model:  $ŷ = w x + b$ <br><br>This is a fully-fledged model. It says, "Price ($ŷ$) is approximately equal to area ($x$) multiplied by some coefficient ($w$), plus some shift ($b$)." <br><br>If you rewrite this in PHP, you get almost trivial code:',
    'what_is_model.description2' => 'Predicting the price of an apartment based on its area using a linear function: $ŷ = w x + b$',
    'what_is_model.explanation_simple' => 'Explanation: $2 * 3 + 0 = 6$<br>By formula: $ŷ = 2 x + b$',
    'what_is_model.intro' => 'When we talk about a model in machine learning, it helps to forget about all the hype around "artificial intelligence" and complex abstractions. In the mathematical sense, a model is just a function: it takes some input data and returns a result. The key difference is that this function is not fixed once and for all — it has parameters that we can tune.',
    'what_is_model.link_function_as_model' => 'Function as the basis of the model',
    'what_is_model.link_error_as_quality' => 'Error as a measure of quality',
    'what_is_model.link_learning_as_min_error' => 'Learning as minimizing error',

    'what_is_model.learning_as_min_error.title' => 'Learning as minimizing error',
    'what_is_model.learning_as_min_error.intro1' => 'If we can measure error (loss), then model training becomes very simple: we change the model parameters so that this number goes down. The model itself does not “understand” the task — it just minimizes the loss function we chose.',
    'what_is_model.learning_as_min_error.intro2' => 'Below is a minimal example: two observations and a linear model $ŷ = w x + b$. First the parameters are bad and the loss is large. Then we adjust $w$ so that predictions become closer to reality and the loss decreases noticeably.',
    'what_is_model.learning_as_min_error.explanation' => 'Training idea: repeat parameter update steps (for example, using gradient descent) until the average error on the data becomes small enough.',
    'what_is_model.error_measure.intro1' => 'Error is a function (commonly called a loss function) that compares the model’s prediction with the true value and returns a number that shows how wrong we were. The smaller this number, the better the model. For example, the simplest error is the difference between prediction and reality: $ŷ - y$.',
    'what_is_model.error_measure.intro2' => 'In practice, we often use the squared error (Squared Error or SE), because it is always non‑negative and penalizes large mistakes more strongly: $(ŷ - y)^2$.',
    'what_is_model.error_measure.explanation' => 'Explanation: −3 ⇒ 7 − 10 = −3<br>Explanation: 4 ⇒ (6 − 4)² = 2² = 4',

    'errors_loss.heading' => 'Error, loss functions, and why they are needed',
    'errors_loss.intro' => 'Any machine learning model boils down to a simple idea: it tries to approximate reality with a function. This means there will always be a gap between what is really happening and what the model says. We call this gap the error. It is important to understand one thing right away: the model does not know what "good" or "bad" means. It does not understand the meaning of the task. All it can do is reduce the number we give it – this number is the loss. Formally, the error is the deviation between $y$ and $\\hat{y}$, and the loss is a function that turns this deviation into a scalar that is convenient to optimize.',
    'errors_loss.intro2' => 'Below we will go through several short but illustrative PHP cases that step by step connect the formulas with real‑world tasks:',
    'errors_loss.case1_title' => 'Case 1. MSE and the cost of a big miss',
    'errors_loss.case1.intro1' => 'Imagine a service that estimates apartment prices. Nothing too fancy: we input the square footage and get a predicted price. This is a classic regression task, and MSE is almost the default choice of loss function. But this is exactly where we can clearly see the price we pay for that choice.',
    'errors_loss.case1.intro2' => 'We can implement MSE in just a few lines of code, without any libraries, and then ruin the whole picture with a single data point. Suppose our dataset now contains a strange apartment: it could be a data error, a unique property, or simply a very bad prediction.',
    'errors_loss.case1.mse_description' => 'MSE (Mean Squared Error) is one of the most common loss functions for regression tasks. It measures the average squared difference between the model prediction and the true value:<br><br>$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$',
    'errors_loss.case1.explanation' => 'Normal MSE: $4$<br>After adding the outlier, the MSE jumps to 4820.<br>This happens because of a single term in the sum (for the outlier): $(300 - 130)^2 = 170^2 = 28900$.',
    'errors_loss.case2_title' => 'Case 2. Choosing a model via the loss function',
    'errors_loss.case2.block_intro' => 'This case is a logical continuation of the previous one. There we looked at how a single bad data point can distort the picture; here we answer a different practical question: how to formally choose the better model when there are several options and they all look almost the same by eye.<br><br><b>Case goal</b>:<br>To show that a loss function turns the subjective "this model seems better" into a measurable criterion that we can actually use to make a decision.<br><br><b>Scenario</b>:<br>Imagine a product demand forecasting task. We have historical data and two model variants:',
    'errors_loss.case2.model_a' => 'Model A – a simple linear model, interpretable and stable',
    'errors_loss.case2.model_b' => 'Model B – a slightly more complex model with additional parameters',
    'errors_loss.case2.models_trained_text' => 'Both models are already trained. We do not discuss their internal structure here – in this case only one thing matters: which of them makes fewer mistakes.',
    'errors_loss.case2.result_paragraph1' => 'After computing the MSE we get two concrete numbers. One of them is smaller – and this is the only formal argument that really matters for training and model selection.',
    'errors_loss.case2.result_paragraph2' => 'Even if the difference between the MSE values is small, it still reflects a systematic advantage of one model over the other within the chosen philosophy of error.',
    'errors_loss.case2.result_paragraph3' => 'Even if the curves look almost identical visually, the loss gives a numerical basis for making a choice.',
    'errors_loss.case3_title' => 'Case 3. Log loss and classifier confidence',
    'errors_loss.case3.intro1' => 'In classification tasks, we often care not only about what class the model predicts, but also about how confident it is in that prediction. Log loss (cross-entropy loss) is a natural way to penalize overconfident mistakes and reward well-calibrated probabilities.',
    'errors_loss.case3.intro2' => 'Two models may give the same final class labels after thresholding at 0.5, but one of them can still be much better calibrated. Log loss makes this difference visible in a single number.',
    'errors_loss.case3.logloss_description' => 'For binary classification, log loss is defined as:<br><br>$\text{LogLoss} = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(p_i) + (1 - y_i) \log(1 - p_i)]$, where $y_i$ is the true label (0 or 1) and $p_i$ is the predicted probability of class 1.',
    'errors_loss.case3.explanation' => 'Even when the predicted class labels coincide, a model that assigns probabilities closer to the true outcomes will achieve a lower log loss. Overconfident wrong predictions are punished especially strongly.',
    'errors_loss.case3.per_sample_heading' => 'Per-sample contribution to Log loss (model A)',
    'errors_loss.case3.curve_heading' => 'Log loss as a function of probability (y = 1)',
    'errors_loss.case3.per_sample_dataset_label' => 'Per-sample Log loss',
    'errors_loss.case3.sample_index_label' => 'Sample index',
    'errors_loss.case4_title' => 'Case 4. Same accuracy – different log loss',
    'errors_loss.case4.intro1' => 'Accuracy answers only one question: how often the model predicts the correct class at a chosen threshold (for example, 0.5). But it does not capture how confident the model is in its predictions.',
    'errors_loss.case4.intro2' => 'In this case we will use the same true labels and two models that yield the same class labels at the 0.5 threshold. However, one model outputs probabilities closer to 0 and 1 and achieves a lower log loss, while the other stays closer to 0.5 and gets penalized with a higher log loss.',
    'errors_loss.case4.explanation' => 'Conclusion: two models can have the same accuracy but different log loss. Log loss evaluates the quality of the predicted probabilities (calibration) and penalizes confident mistakes much more strongly.',
    'errors_loss.case5_title' => 'Case 5. Training a model as minimizing error',

    'linear_regression.heading' => 'Linear regression as a basic model',
    'linear_regression.intro1' => 'Linear regression is a natural starting point for talking about machine learning. Not because it is "simple", but because it already contains almost everything: the model as a function, parameters, error, optimization, and geometric interpretation. If you understand linear regression, most other models will be perceived as its complications.',
    'linear_regression.intro2' => 'In the following cases we will treat linear regression not as a toy teaching algorithm, but as an engineering tool.',
    'linear_regression.case1_title' => 'Case 1. Apartment valuation based on parameters',
    'linear_regression.case2_title' => 'Case 2. Predicting a developer’s task completion time',
    'linear_regression.case3_title' => 'Case 3. Predicting server resource consumption',
    'linear_regression.case4_title' => 'Case 4. Estimating the likely customer check',
    'linear_regression.case5_title' => 'Case 5. Predicting market salary',
    'linear_regression.case1.php_impl_intro' => 'We will start without any libraries. This is useful not for production, but for building intuition. We will use gradient descent, a feature matrix $X$ of size $N$ x $4$, and a weight vector $w$ of length $4$. We will add the bias as an extra feature with value 1.',
    'linear_regression.case1.rubix_impl_title' => 'Implementation with RubixML',
    'linear_regression.case1.rubix_impl_intro' => 'Now we will do the same thing, but the way it is usually done in real projects. We will use linear regression trained with the least squares method. The library itself will solve the optimization problem and find the weights analytically.',
    'linear_regression.case1.php_result_intro' => 'The block above shows the result of the script: the predicted apartment price for the following features:',
    'linear_regression.case1.feature_area' => 'Apartment area: 60 m²',
    'linear_regression.case1.feature_rooms' => 'Number of rooms: 5',
    'linear_regression.case1.feature_bathrooms' => 'Number of bathrooms: 4',
    'linear_regression.case1.feature_floors' => 'Number of floors: 12',
    'linear_regression.case1.feature_bias' => 'Deviation: 1',

    // Linear regression: Case 2 (developer task completion time)
    'linear_regression.case2.intro1' => 'In real teams, the question "how long will this task take?" comes up almost every day. Release dates, developer workload, and business expectations all depend on it. Estimates are usually given by gut feeling — based on the team lead’s or the developer’s own experience. Linear regression lets us formalize this process and build a baseline model that predicts time based on task features.',
    'linear_regression.case2.intro2' => 'Suppose we have historical task data: for each task we know how many hours it actually took and a feature vector $\\mathbf{x} = (x_1, x_2, x_3, x_4)$, where:',
    'linear_regression.case2.feature_x1' => '$x_1$ — story points (a measure of task complexity);',
    'linear_regression.case2.feature_x2' => '$x_2$ — number of files touched by the change;',
    'linear_regression.case2.feature_x3' => '$x_3$ — number of changed lines (code diff);',
    'linear_regression.case2.feature_x4' => '$x_4$ — developer experience (for example, in years or as an encoded level junior/middle/senior);',
    'linear_regression.case2.formula' => 'We want to predict the target quantity – the actual task completion time in hours. Linear regression in this case defines a simple, interpretable model $ŷ = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4$, where the components of $\\mathbf{x}$ describe the task and the coefficients $w_1, \\dots, w_4$ and intercept $w_0$ are fitted on historical data.',

    'linear_regression.case2.rubix_intro' => 'In this example we use Ridge linear regression from RubixML to predict a developer’s task completion time based on several numeric features and at the same time inspect the model weights and bias.',
    'linear_regression.case2.explain_intro' => 'Now the model is explainable (a weight is a coefficient):',
    'linear_regression.case2.explain_item1' => 'the weight at story points shows how many hours on average one additional SP adds;',
    'linear_regression.case2.explain_item2' => 'the weight at the number of files reflects context-switching overhead;',
    'linear_regression.case2.explain_item3' => 'the weight at the number of lines often correlates with the amount of manual work;',
    'linear_regression.case2.explain_item4' => 'a negative weight at developer experience is expected and logical (the more experience, the less time is usually needed for the same task, so the relationship is inverse and the weight becomes negative);',
    'linear_regression.case2.explain_outro' => 'This kind of model can already be discussed with the team, and the feature set can be adjusted consciously.',

    // Linear regression: Case 3 (server resource consumption forecast)
    'linear_regression.case3.intro1' => 'In production, it is often not enough to know only the response time — we also care whether the infrastructure will handle the load: how much CPU usage will grow, whether we will hit memory limits, whether the database will start choking. Linear regression lets us build a quick, interpretable model that predicts server load from a few simple traffic metrics.',
    'linear_regression.case3.intro2' => 'Suppose we have historical monitoring data. For each time window we know a feature vector $\\mathbf{x} = (x_1, x_2, x_3, x_4, x_5)$, where:',
    'linear_regression.case3.feature_x1' => '$x_1$ — requests per minute;',
    'linear_regression.case3.feature_x2' => '$x_2$ — average response size (KB);',
    'linear_regression.case3.feature_x3' => '$x_3$ — number of active users on the site;',
    'linear_regression.case3.feature_x4' => '$x_4$ — number of background cron jobs in the interval;',
    'linear_regression.case3.feature_x5' => '$x_5$ — hour of day (for example, from 0 to 23);',
    'linear_regression.case3.formula' => 'The target variable is CPU load in percent. Linear regression in this case defines a model $ŷ = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4 + w_5 x_5$, where the coefficients $w_1, \\dots, w_5$ and intercept $w_0$ are fitted using historical monitoring data.',
    'linear_regression.case3.rubix_intro' => 'In this example we use linear regression from RubixML to predict CPU load from a small set of traffic metrics and at the same time inspect the model weights and bias.',
    'linear_regression.case3.explain_intro' => 'After training, the model can be interpreted similarly to the task‑time case:',
    'linear_regression.case3.explain_item1' => 'the weight at requests per minute shows how sensitive CPU is to increased incoming traffic;',
    'linear_regression.case3.explain_item2' => 'the weight at average response size reflects the impact of "heavy" responses (template rendering, large JSON/HTML);',
    'linear_regression.case3.explain_item3' => 'the weight at the number of active users implicitly accounts for concurrent sessions and competition for resources;',
    'linear_regression.case3.explain_item4' => 'the weight at the number of cron jobs captures the impact of background load (backups, reports, recalculations);',
    'linear_regression.case3.explain_item5' => 'the weight at hour of day allows the model to account for daily load patterns (peaks during the day, dips at night);',
    'linear_regression.case3.explain_outro' => 'Such a model is convenient as a quick estimate of "will the server survive another +X% traffic" and as a starting point for more complex models and alerts in the monitoring system.',

    // Linear regression: Case 4 (estimating the likely customer check)
    'linear_regression.case4.intro1' => 'In e-commerce, marketplaces, and subscription services it is often useful to estimate a customer’s likely purchase amount (“check”) in advance. This helps with personalization, discount strategy, and prioritizing user segments.',
    'linear_regression.case4.intro2' => 'Below is a minimal baseline regression model that predicts the likely check from simple on-site behavior features. To keep predictions positive and to make the target distribution more stable, we predict the logarithm of the check and then convert it back to money with exp().',
    'linear_regression.case4.feature_x1' => '$x_1$ — number of visits in the period;',
    'linear_regression.case4.feature_x2' => '$x_2$ — time on site (seconds);',
    'linear_regression.case4.feature_x3' => '$x_3$ — number of pageviews;',
    'linear_regression.case4.feature_x4' => '$x_4$ — discount (percent);',
    'linear_regression.case4.formula' => 'The target variable is the log-check: $y = \log(\text{check})$. The model predicts $\hat{y}$ and we restore the check as $\widehat{\text{check}} = \exp(\hat{y})$.',
    'linear_regression.case4.rubix_intro' => 'In this example we use Ridge regression from RubixML. The model is trained on a small dataset, predicts log(check), and then we convert the result back to money with exp().',
    'linear_regression.case4.explain_intro' => 'Why predicting log(check) is a useful trick:',
    'linear_regression.case4.explain_item1' => 'checks often have a heavy tail; log() reduces the impact of rare large purchases;',
    'linear_regression.case4.explain_item2' => 'in log-space, the model often better matches multiplicative effects (e.g. “twice as many pageviews”);',
    'linear_regression.case4.explain_item3' => 'exp() guarantees a positive predicted check;',
    'linear_regression.case4.explain_item4' => 'Ridge adds L2 regularization and helps reduce overfitting on small datasets.',
    'linear_regression.case4.explain_outro' => 'In a real product you would expand the dataset and feature set (traffic source, category interest, purchase history), but even this baseline can be a good starting point.',

    // Case 1 charts UI (code-run.php)
    'linear_regression.case1.chart_price_vs_area' => 'Price vs Area',
    'linear_regression.case1.chart_price_vs_floor' => 'Price vs Floor',
    'linear_regression.case1.chart_price_vs_distance' => 'Price vs Distance',
    'linear_regression.case1.chart_price_vs_age' => 'Price vs Age',
    'linear_regression.case1.chart_xlabel_area' => 'Area (sq.m)',
    'linear_regression.case1.chart_xlabel_floor' => 'Floor',
    'linear_regression.case1.chart_xlabel_distance' => 'Distance to City Center (km)',
    'linear_regression.case1.chart_xlabel_age' => 'Building Age (years)',
    'linear_regression.case1.chart_ylabel_price' => 'Price ($)',
    'linear_regression.case1.chart_regression_label' => 'Regression Line',
    'linear_regression.case1.controls_chart_type' => 'Chart Type',

    'gradient_descent.heading' => 'Gradient descent on fingers',
    'gradient_descent.implementation' => 'Implementation of gradient descent',
    'gradient_descent.minimal_example_intro' => 'Let’s start with a minimal example: one feature, one weight. Estimating an apartment price by its area.',
    'gradient_descent.intro1' => 'Gradient descent (the gradient descent method) is a numerical method for finding a local minimum or maximum of a function by moving along the gradient; it is one of the core numerical methods in modern optimization.',
    'gradient_descent.intro2' => 'When people in machine learning talk about "training a model", they almost always mean the same thing: we want to choose parameters so that the error becomes as small as possible. Which exact model it is — linear regression, logistic regression, a neural network — is less important. More important is that under the hood the same mechanism almost always works: gradient descent.',
    'gradient_descent.implementation_link' => 'Implementation of gradient descent',
    'gradient_descent.sample1_title' => 'Example 1. Parameter trajectory',
    'gradient_descent.sample2_title' => 'Example 2. Effect of learning rate',
    'gradient_descent.sample3_title' => 'Example 3. Plateau and near-zero gradient',
    'gradient_descent.sample4_title' => 'Example 4. Batch and stochastic descent',
    'gradient_descent.sample1.goal' => 'The goal of this example is to see how the parameter $w$ changes from epoch to epoch during training.',
    'gradient_descent.sample1.data_label' => 'Data:',
    'gradient_descent.sample1.model_label' => 'Model:',
    'gradient_descent.sample1.update_label' => 'Parameter update:',
    'gradient_descent.sample1.run_intro' => 'The table below shows the trajectory of parameter $w$ during batch gradient descent on the data $y = 2x$.',
    'gradient_descent.sample1.ui.epoch' => 'Epoch',
    'gradient_descent.sample1.ui.play' => 'Play',
    'gradient_descent.sample1.ui.pause' => 'Pause',
    'gradient_descent.sample1.ui.speed' => 'Speed',
    'gradient_descent.sample1.ui.loss' => 'loss',
    'gradient_descent.sample1.ui.current' => 'current point',
    'gradient_descent.sample1.ui.current-loss' => 'current point (loss)',
    'gradient_descent.result_hint' => 'For this data, the result will be close to:',
    'gradient_descent.debug_title' => 'Gradient descent debug',
    'gradient_descent.learning_rate' => 'Learning rate',
    'gradient_descent.epochs' => 'Epochs',
    'gradient_descent.impl_php_from_scratch' => 'Implementation in PHP from scratch',
    'gradient_descent.impl_php_vector_version' => 'Implementation in PHP – vector version',
    'gradient_descent.more_features_vectors_hint' => 'When there are more features, it is more convenient to think in vectors.',

    // Part III: Probability as degree of confidence
    'probability_confidence.intro' => 'When developers hear the word "probability", they often imagine dice, coin flips, and the school formula "favorable outcomes divided by all possible outcomes". This is useful, but a very narrow picture. In machine learning and applied analytics, probability almost always means something else – the degree of our confidence in a statement given the available data.',
    'probability_confidence.link_softmax_example' => 'Probability example with softmax',
    'probability_confidence.case1_title' => 'Case 1. Spam filter: probability ≠ decision',
    'probability_confidence.case2_title' => 'Case 2. Medical test: updating confidence',
    'probability_confidence.case3_title' => 'Case 3. Multiclass classification and softmax',
    'probability_confidence.case4_title' => 'Case 4. Overconfident model as a problem signal',
    'probability_confidence.case5_title' => 'Case 5. Updating confidence with new data',

    'probability_confidence.case2.scenario_title' => 'Scenario',
    'probability_confidence.case2.scenario_intro' => 'Suppose there is a rare disease. We know the following:',
    'probability_confidence.case2.scenario_item1' => 'The disease occurs in 1 person out of 1,000.',
    'probability_confidence.case2.scenario_item2' => 'Test sensitivity (probability of a positive result given the disease) is 99%.',
    'probability_confidence.case2.scenario_item3' => 'Test specificity (probability of a negative result given a healthy person) is 95%.',
    'probability_confidence.case2.scenario_question' => 'A patient takes the test and gets a positive result. Question: what is the probability that they truly have the disease?',
    'probability_confidence.case2.scenario_note' => 'An intuitive answer often sounds like “about 99%”. But that is wrong. The reason is the rarity of the event.',

    'probability_confidence.logits_paragraph' => 'In many machine learning models, the outputs are not probabilities, but so‑called scores (logits). These are just numbers that reflect the model\'s relative confidence in each option. They can be anything — positive, negative, large or small — and by themselves they are not interpreted as probabilities. To turn such scores into proper probabilities, we use the softmax function.',
    'probability_confidence.softmax_result_explanation' => 'Now we have a correct probability distribution: each value lies in the range from 0 to 1; the sum of all values equals 1; and the numbers can be interpreted as the model\'s degree of confidence.',

    // Part III: Logistic regression cases
    'logistic_regression.intro' => 'Up to this point, we have talked about logistic regression as a model: formula, sigmoid, probability, decision boundary. All of this is important, but by itself remains theory. Cases are needed to show how this model works on real tasks. There will be no "perfect" examples here. The data may be simple or noisy, the features obvious or strange, the decisions not always clear-cut. That is normal. This is exactly how logistic regression is used in practice.',
    'logistic_regression.case1.php_intro' => 'Customer churn is one of the most typical binary classification tasks. A user either stays in the product or leaves. What we really care about is not just the final "yes or no", but the probability of churn: how high the risk is and whether we should react.',
    'logistic_regression.case1.php_intro2' => 'This is exactly where logistic regression fits especially well. We will build a simple model in pure PHP that estimates the churn probability from user behavior and see how this probability is formed from features.',
    'logistic_regression.case1_title' => 'Case 1. Logistic regression for customer churn',
    'logistic_regression.case2_title' => 'Case 2. Newsletter subscription',
    'logistic_regression.case3_title' => 'Case 3. Spam or not spam',
    'logistic_regression.case4_title' => 'Case 4. Ad click (CTR)',
    'logistic_regression.case5_title' => 'Case 5. Loan approval',
    'logistic_regression.case6_title' => 'Case 6. Fraud vs normal transaction',
    'logistic_regression.case7_title' => 'Case 7. Medical screening',
    'logistic_regression.case8_title' => 'Case 8. Equipment technical failure',
    'logistic_regression.case1.rubix_intro' => 'In real projects, people rarely implement logistic regression from scratch. It is much more convenient to use existing libraries.',
    'logistic_regression.case1.rubix_intro2' => 'The very same churn case implemented with RubixML becomes noticeably shorter and closer to how it is often done in production:',
    'logistic_regression.case2.intro' => 'There is no multidimensional space, no complex features, and no intersecting factors. There is one feature and a binary decision. And this is already enough to see the entire logic of logistic regression.<br><br><b>Case Goal</b><br>Predict whether a user will subscribe to an email newsletter based only on the time spent on the site.<br><br>The model should answer two questions:<br>1) What is the probability of subscription?<br>2) Where is the boundary between "most likely to subscribe" and "most likely not to subscribe"?',
    'why_naive_bayes_works.title' => 'Why Naive Bayes works',
    'why_naive_bayes_works.case1.title' => 'Case 1. Categorical features and frequencies',
    'why_naive_bayes_works.case1.php_intro' => 'In this case we look at the simplest Naive Bayes with categorical features. We will explicitly count per-class frequencies, convert them into probabilities, and classify a new user.',
    'why_naive_bayes_works.case1.rubix_intro' => 'Then we implement the same example using RubixML and see how the same logic is expressed at the library level.',
    'why_naive_bayes_works.case1.php_run_intro' => 'In this example we implement Naive Bayes by hand for a pair of simple categorical features and see what log-probability scores we get for each class.',
    'why_naive_bayes_works.case1.php_result_explanation' => 'The output is shown as <strong>logarithms</strong> of the probability scores for each class. The larger the value (i.e. closer to 0), the more likely the class. We sum log-probabilities per feature and apply Laplace smoothing, so after sorting the first class is the predicted one.',
    'why_naive_bayes_works.case1.rubix_run_intro' => 'Here the same case is implemented with RubixML: we train Naive Bayes on the same data and see which class the model assigns to a new sample.',
    'why_naive_bayes_works.case1.rubix_result_explanation' => 'RubixML returns the predicted class label for the given sample. Under the hood it applies the same Naive Bayes idea (class prior × conditional likelihoods), so the result should match the pure PHP implementation for the same data.',

    'why_naive_bayes_works.case2.title' => 'Case 2. Word-based spam filter (Bernoulli Naive Bayes)',
    'why_naive_bayes_works.case2.php_intro' => 'In this case we build a <strong>Bernoulli Naive Bayes</strong> spam filter for text. Each word becomes a binary feature: present (1) or absent (0). Unlike the multinomial variant, the Bernoulli model also explicitly accounts for missing words, so absence contributes to the score too.',
    'why_naive_bayes_works.case2.rubix_intro' => 'Then we implement the same idea using RubixML: create binary features (word present/absent), train Naive Bayes, and predict the class for a new email.',
    'why_naive_bayes_works.case2.php_run_intro' => 'Below is the runnable code: we count class priors, build the vocabulary, apply Bernoulli Laplace smoothing, and compute log-scores for spam/ham.',
    'why_naive_bayes_works.case2.php_result_explanation' => 'The output is the log score for each class. The higher the value (closer to 0), the more likely the class. After sorting, the predicted class will come first.<br><br><b>How to read the output</b><br>Comparison:<br><br>spam = {spam}<br>ham = {ham}<br><br>{spam} > {ham} → the model selects spam',
    'why_naive_bayes_works.case2.rubix_run_intro' => 'Here we train RubixML NaiveBayes on binary bag-of-words features (0/1) and predict the class for a new word vector.',
    'why_naive_bayes_works.case2.rubix_result_explanation' => 'RubixML returns the predicted label for the given vector. Conceptually it is the same scheme: prior × likelihoods (smoothing and log-space computations are handled by the library).',

    // Part IV: k-NN and local solutions
    'knn_local_solutions.index.intro' => 'The k-nearest neighbors (k-NN) algorithm is one of the most intuitive yet fundamental machine learning algorithms. It makes almost no assumptions about the data, does not train a parametric model, but instead stores the training examples and relies on a simple, almost geometric idea: similar objects should have similar answers.',
    'knn_local_solutions.index.case1' => 'Case 1. Customer classification by behavior',
    'knn_local_solutions.index.case2' => 'Case 2. Regression: apartment price estimation',
    'knn_local_solutions.index.case3' => 'Case 3. Classification with RubixML',
    'knn_local_solutions.case1.intro' => 'In this case, we will walk through one of the simplest and most illustrative applications of k-nearest neighbors: classifying a user based on their on-site behavior. The example is intentionally simplified so you can focus on the decision logic rather than infrastructure.',
    'knn_local_solutions.case2.intro' => 'In this case, we apply the same k-nearest neighbors idea to a regression task. Instead of predicting a class label, we estimate a numeric value: the price of an apartment. We compute distances to known examples, take the k closest ones, and average their prices.',

    'decision_trees_space_partitioning.index.intro' => 'Decision trees are one of the most intuitive and at the same time powerful machine learning algorithms. They are loved for their clarity, for being close to the human “if–then” logic, and for the fact that the result can be explained not only to an engineer, but also to the business, a manager, or a client. Unlike many other models, a decision tree does not look like a black box — it can literally be drawn on paper.',
    'decision_trees_space_partitioning.index.case1' => 'Case 1. A tutorial decision tree in pure PHP',
    'decision_trees_space_partitioning.index.case2' => 'Case 2. Classification using RubixML',
    'decision_trees_space_partitioning.index.case3' => 'Case 3. When a tree is convenient in a real product',
    'decision_trees_space_partitioning.case1.intro' => 'In this case we build a minimal tutorial decision tree: we compute entropy and information gain for a simple split, pick the best threshold for a feature, and show how the dataset is split into left and right branches.',

    'decision_trees_space_partitioning.case2.intro' => 'In the previous case we explored a tutorial decision tree implementation in pure PHP and saw how the algorithm works inside. Now we will solve the same task using a production-ready library. The goal of this case is to show how the familiar entropy and information gain logic is applied in a real tool and how a decision tree is integrated into a typical PHP project.',
    'decision_trees_space_partitioning.case2.rubix_run_intro' => 'Below is the runnable code: we build a Labeled dataset, train a ClassificationTree, and predict the class for a new sample.',
    'decision_trees_space_partitioning.case2.rubix_result_explanation' => 'RubixML returns the predicted class label for the input vector. Conceptually it is the same split-based approach with a quality criterion (entropy / information gain), but the tree building and split selection are handled internally by the library.',
    'decision_trees_space_partitioning.case1.diagram_default' => 'Decision Tree',
    'decision_trees_space_partitioning.case1.diagram_all_data' => 'All data',
    'decision_trees_space_partitioning.case1.diagram_left_branch' => 'Left branch',
    'decision_trees_space_partitioning.case1.diagram_right_branch' => 'Right branch',
    'decision_trees_space_partitioning.case1.diagram_yes' => 'Yes',
    'decision_trees_space_partitioning.case1.diagram_no' => 'No',
    'decision_trees_space_partitioning.case1.diagram_graph_label' => 'Graph',
    'decision_trees_space_partitioning.case1.diagram_class_label' => 'Class',

    'hands_on_embedding_in_php_with_transformers.index.intro' => 'In this section we move from theory to practice: we take embeddings and apply them to several typical product tasks. Each case is a small engineering pattern you can reuse in PHP projects.',
    'hands_on_embedding_in_php_with_transformers.index.case1' => 'Case 1. Semantic search over text documents (no DB)',
    'hands_on_embedding_in_php_with_transformers.index.case2' => 'Case 2. Similar records search (deduplication / near-duplicates)',
    'hands_on_embedding_in_php_with_transformers.index.case3' => 'Case 3. Semantic FAQ / knowledge base search',
    'hands_on_embedding_in_php_with_transformers.index.case4' => 'Case 4. Intelligent navigation over events / timelines',
    'hands_on_embedding_in_php_with_transformers.index.case5' => 'Case 5. Classification without training (zero-shot via similarity)',
    'hands_on_embedding_in_php_with_transformers.index.case6' => 'Case 6. Semantic recommendations “similar articles”',

    'hands_on_embedding_in_php_with_transformers.case4.title' => 'Case 4. Intelligent navigation over events / timelines',
    'hands_on_embedding_in_php_with_transformers.case4.intro' => 'Goal: quickly find the most relevant events in a timeline by meaning, not by exact keyword matches. We compute embeddings for event descriptions, cache them to disk, and rank events using cosine similarity against the query.',

    'rag_engineering_system.index.intro' => 'This section builds the engineering foundation for RAG: how text becomes numbers, how retrieval works, why embeddings and transformers matter, and how to think about RAG as a system you can build, measure, and improve.',
    'rag_engineering_system.index.item1' => 'Why words turn into numbers: word spaces and features',
    'rag_engineering_system.index.item2' => 'Bag of Words and TF–IDF',
    'rag_engineering_system.index.item3' => 'Embeddings as continuous spaces of meaning',
    'rag_engineering_system.index.item4' => 'Transformers and context: from static vectors to understanding meaning',
    'rag_engineering_system.index.item5' => 'Hands-on: embeddings in PHP with transformers (inference, not training)',
    'rag_engineering_system.index.item6' => 'RAG: Retrieval-Augmented Generation as an engineering system',

    'rag_engineering_system.why_do_words_turn_into_numbers.title' => 'Why words turn into numbers: word spaces and features',
    'rag_engineering_system.why_do_words_turn_into_numbers.intro' => 'We revisit the core idea: to work with text algorithmically we represent it as numbers. Here we focus on the notion of feature spaces, what a “word space” means, and why the choice of representation defines what the system can and cannot learn.',

    'rag_engineering_system.bag_of_words_and_tf_idf.title' => 'Bag of Words and TF–IDF',
    'rag_engineering_system.bag_of_words_and_tf_idf.intro' => 'We cover sparse representations: Bag of Words, term frequency, inverse document frequency, and how TF–IDF becomes a simple baseline for retrieval. This is where the “retrieval” part of RAG historically starts.',

    'rag_engineering_system.embeddings_as_continuous_spaces_of_meaning.title' => 'Embeddings as continuous spaces of meaning',
    'rag_engineering_system.embeddings_as_continuous_spaces_of_meaning.intro' => 'We move from sparse vectors to dense embeddings and discuss why continuous vector spaces capture semantics better: similarity search, clustering, deduplication, and the practical trade-offs (model choice, dimensionality, normalization, distance metrics).',

    'rag_engineering_system.transformers_and_context.title' => 'Transformers and context: from static vectors to understanding meaning',
    'rag_engineering_system.transformers_and_context.intro' => 'We connect embeddings to transformers: how contextual representations differ from static vectors, what “context window” really is, and why modern RAG systems rely on transformer inference pipelines.',

    'rag_engineering_system.hands_on_embeddings_in_php_with_transformers.title' => 'Hands-on: embeddings in PHP with transformers (inference, not training)',
    'rag_engineering_system.hands_on_embeddings_in_php_with_transformers.intro' => 'We discuss the engineering approach: do inference instead of training, treat embeddings as a reusable service, and think in terms of API contracts, caching, batching, and cost. In practice this is where TransformersPHP becomes a tool.',

    'rag_engineering_system.rag_as_engineering_system.title' => 'RAG: Retrieval-Augmented Generation as an engineering system',
    'rag_engineering_system.rag_as_engineering_system.intro' => 'We put everything together: ingestion, chunking, embedding, indexing, retrieval, reranking, prompting, and evaluation. The focus is not on “one trick” but on building a reliable system: latency, quality, observability, and iteration loops.',

    'bag_of_words_and_tf_idf.simple_tfidf_example' => 'A simple TF–IDF example in PHP',
    'bag_of_words_and_tf_idf.simple_tfidf_example_intro' => 'Below is a minimal TF–IDF implementation in pure PHP. We take three short documents (about a cat and a dog), build a vocabulary, compute TF and IDF, and then produce TF–IDF weights for each term in each document.',

    'bag_of_words_and_tf_idf.index.intro' => 'In the previous chapters we talked about text as data and about the fact that a computer cannot read words “like a human”. For it, text is a set of symbols, numbers, and statistics. In this section we will cover two basic, yet still extremely useful approaches to representing text as numbers: Bag of Words and TF–IDF.',
    'bag_of_words_and_tf_idf.index.item_simple_tfidf' => 'A simple TF–IDF example in PHP',
    'bag_of_words_and_tf_idf.index.case1' => 'Case 1. Similar document search',
    'bag_of_words_and_tf_idf.index.case2' => 'Case 2. Review classification: “positive / negative”',
    'bag_of_words_and_tf_idf.index.case3' => 'Case 3. Automatic article categorization',
    'bag_of_words_and_tf_idf.index.case4' => 'Case 4. “Spam” detector for contact forms',
    'bag_of_words_and_tf_idf.index.case5' => 'Case 5. Explainable search: “why this document?”',
    'bag_of_words_and_tf_idf.index.case6' => 'Case 6. Comparison: Bag of Words vs TF–IDF on one example',

    'why_do_words_turn_into_numbers.index.note_no_code' => 'There are no runnable examples or code in this chapter because it is a concept, not a hands-on implementation.',
];
